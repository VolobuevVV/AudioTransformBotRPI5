import torch
from telegram import Update, InputFile, ReplyKeyboardMarkup
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext
from pydub import AudioSegment
import os
import whisper

BOT_TOKEN = '8101388926:AAEjCS7kwSp8EitsYo8m11rT4SeQzUsSf4M'

user_models = {}
loaded_models = {
    "tiny": whisper.load_model("tiny"),
    "base": whisper.load_model("base"),
    "small": whisper.load_model("small")
}

def main_menu_keyboard():
    return ReplyKeyboardMarkup([['–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≥–æ–ª–æ—Å', '–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å']], resize_keyboard=True)

def model_selection_keyboard():
    return ReplyKeyboardMarkup([['tiny', 'base', 'small'], ['–ù–∞–∑–∞–¥']], resize_keyboard=True)

def back_keyboard():
    return ReplyKeyboardMarkup([['–ù–∞–∑–∞–¥']], resize_keyboard=True)

def start(update: Update, context: CallbackContext):
    context.user_data.clear()
    user_name = update.message.from_user.first_name
    update.message.reply_text(f'–ü—Ä–∏–≤–µ—Ç, {user_name}! üòä –í—ã–±–µ—Ä–∏ –æ–¥–Ω—É –∏–∑ –æ–ø—Ü–∏–π:', reply_markup=main_menu_keyboard())

def handle_text(update: Update, context: CallbackContext):
    text = update.message.text
    user_id = update.message.from_user.id
    step = context.user_data.get("step")

    if text == '–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å':
        context.user_data['step'] = 'select_model'
        update.message.reply_text("–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å Whisper –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:", reply_markup=model_selection_keyboard())

    elif text in ['tiny', 'base', 'small'] and step == 'select_model':
        user_models[user_id] = text
        context.user_data['model'] = text
        context.user_data['step'] = 'awaiting_recognition_voice'
        update.message.reply_text(f"–í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {text}. –¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ üß†", reply_markup=back_keyboard())

    elif text == '–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≥–æ–ª–æ—Å':
        context.user_data['step'] = 'awaiting_transform_voice'
        update.message.reply_text("–û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ üéµ", reply_markup=back_keyboard())

    elif text == '–ù–∞–∑–∞–¥':
        if step == 'awaiting_recognition_voice':
            context.user_data['step'] = 'select_model'
            update.message.reply_text("–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å Whisper –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:", reply_markup=model_selection_keyboard())
        elif step == 'select_model' or step == 'awaiting_transform_voice':
            context.user_data.clear()
            update.message.reply_text("–í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é:", reply_markup=main_menu_keyboard())
        else:
            update.message.reply_text("–¢—ã —É–∂–µ –≤ –≥–ª–∞–≤–Ω–æ–º –º–µ–Ω—é üòä", reply_markup=main_menu_keyboard())

    else:
        update.message.reply_text("–Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é —ç—Ç—É –∫–æ–º–∞–Ω–¥—É üòÖ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π –∫–Ω–æ–ø–∫–∏.")

def transcribe_audio(audio_file, model_name):
    model = loaded_models[model_name]
    result = model.transcribe(audio_file, language="ru")
    return result["text"]

def voice(update: Update, context: CallbackContext):
    step = context.user_data.get('step')
    user_id = update.message.from_user.id
    model_name = user_models.get(user_id, 'tiny')

    file = update.message.voice.get_file()
    file_path = "voice.ogg"
    file.download(file_path)

    if step == 'awaiting_transform_voice':
        sound = AudioSegment.from_ogg(file_path)
        octaves = -0.5
        new_sample_rate = int(sound.frame_rate * (2 ** octaves))
        sound = sound._spawn(sound.raw_data, overrides={'frame_rate': new_sample_rate})
        sound = sound.set_frame_rate(44100).set_channels(1)

        output_ogg_path = "voice_lowered.ogg"
        output_wav_path = "voice_lowered.wav"
        sound.export(output_ogg_path, format="ogg")
        sound.export(output_wav_path, format="wav")

        with open(output_ogg_path, 'rb') as f:
            update.message.reply_voice(voice=InputFile(f), caption="–í–æ—Ç —Ç–≤–æ–π –≥–æ–ª–æ—Å —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º —Ç–æ–Ω–æ–º!")

        with open(output_wav_path, 'rb') as f:
            update.message.reply_document(document=InputFile(f))

        os.remove(output_ogg_path)
        os.remove(output_wav_path)

    elif step == 'awaiting_recognition_voice':
        text = transcribe_audio(file_path, model_name)
        update.message.reply_text(text)

    else:
        update.message.reply_text("–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ —Å –ø–æ–º–æ—â—å—é –∫–Ω–æ–ø–æ–∫.")

    os.remove(file_path)

    context.user_data.clear()
    update.message.reply_text("–í—ã–±–µ—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ:", reply_markup=main_menu_keyboard())

def main():
    updater = Updater(BOT_TOKEN)
    dispatcher = updater.dispatcher
    dispatcher.add_handler(CommandHandler("start", start))
    dispatcher.add_handler(MessageHandler(Filters.voice, voice))
    dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_text))
    updater.start_polling()
    updater.idle()

if __name__ == '__main__':
    main()
